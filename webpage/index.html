<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>OmniSearch</title>
  <link rel="icon" href="https://img.alicdn.com/imgextra/i4/O1CN01FOwagl1XBpyVA2QVy_!!6000000002886-2-tps-512-512.png"/>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <style>
    .findings-box {
      border: 2px solid #d0d9e0;
      border-radius: 8px;
      padding: 10px 15px;
      display: inline-block;
      font-family: Georgia, "Times New Roman", Times, serif;
      font-size: 16px;
      line-height: 1.5;
      background-color: #f9f9f9;
    }

    .findings-box .title {
      font-weight: bold;
      text-decoration: underline;
      font-size: 18px;
    }

    .findings-box .content {
      font-style: italic;
    }
  </style>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a herf="https://callanwu.github.io/">Yangning Li</a><sup>*</sup>,</span>
              <span class="author-block">Yinghui Li,<sup>*</sup>,</span>
              <span class="author-block">Xinyu Wang<sup>&dagger;</sup>,</span>
              <span class="author-block">Yong Jiang<sup>&dagger;</sup>,</span>
              <span class="author-block">Zhen Zhang,</span>
              <span class="author-block">Xinran Zheng,</span>
              <br>
              <span class="author-block">Hui Wang,</span>
              <span class="author-block">Hai-Tao Zheng,</span>
              <span class="author-block">Pengjun Xie,</span>
              <span class="author-block">Philip S. Yu,</span>
              <span class="author-block">Fei Huang,</span>
              <span class="author-block">Jingren Zhou</span>
            </div>

            <div class="is-size-5 publication-authors">
              liyangning.lyn@alibaba-inc.com
              <br>
              <span class="author-block"><b>Tongyi Lab <img src="static/images/tongyi.jpg" alt="Tongyi Logo" style="width: 20px; height: 20px;"/>
                , Alibaba Group</b></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Work done during internship at Tongyi Lab, Alibaba Group.</small></span>
            </div>

                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2411.02937" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/Alibaba-NLP/OmniSearch" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/zhzhen23/DynVQA" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="static/images/hf-logo.png" alt="Hugging Face Logo" style="width: 20px; height: 20px;"/>
                  </span>
                    <span>Dataset</span>
                  </a>
                </span>

                  <span class="link-block">
                      <a href="https://huggingface.co/spaces/zhzhen23/OmniSearchLeaderboard" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="static/images/leaderboard.png" alt="leaderboard Logo" style="width: 20px; height: 25px;"/>
                      </span>
                        <span>Leaderboard</span>
                      </a>
                  </span>

                  <span class="link-block">
                    <a href="https://www.modelscope.cn/studios/iic/OmniSearch" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="static/images/modelscope.ico" alt="Modelscope Logo" style="width: 20px; height: 20px;"/>
                      </span>
                      <span>Demo</span>
                      </a>
                  </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img id="teaser" src="./static/images/ask_test_2.5.gif" alt="Demo of WebWalker" height="100%">

        <h2 class="subtitle has-text-centered">
          <span class="dnerf">
            A demo of WebWalker. You can explore it <a href="https://www.modelscope.cn/studios/iic/OmniSearch" target="_blank">here</a>.
          </span>
        </h2>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Multimodal Retrieval Augmented Generation (mRAG) plays an important role in mitigating the “hallucination” issue inherent in multimodal large language models (MLLMs). Although promising, existing heuristic mRAGs typically predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws cannot be adequately reflected by current knowledge-seeking visual question answering (VQA) datasets, since the most required knowledge can be readily obtained with a standard two-step retrieval. To bridge the dataset gap, we first construct <b style="color:#615ced;">Dyn-VQA</b> dataset, consisting of three types of ``dynamic'' questions, which require complex knowledge retrieval strategies variable in query, tool, and time: (1) Questions with rapidly changing answers. (2) Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient and precisely relevant knowledge for dynamic questions due to their rigid retrieval processes. Hence, we further propose the first self-adaptive planning agent for multimodal retrieval, <b style="color:#615ced;">OmniSearch</b>. The underlying idea is to emulate the human behavior in question solution which dynamically decomposes complex multimodal questions into sub-question chains with retrieval action. Extensive experiments prove the effectiveness of our OmniSearch, also provide direction for advancing mRAG. Code and dataset will be open-sourced.
            </p>
          </div>
         
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Overview -->
  <section class="section" id="Overview">
    <div class="container is-max-desktop content">
      <div class="columns is-centered has-text-centered">
        <div class="column is-five-fifths">
          <h2 class="title is-3">🌟Overview</h2>
          <div class="content has-text-justified">
            <p>
              📚 We reveal that existing VQA-based mRAG benchmarks fail to reflect the feature that realworld questions require dynamic knowledge retrieval, and propose novel <b style="color:#615ced;">Dyn-VQA</b> dataset, which
              contains three types of dynamic questions..
            </p>
            <p>
              💡 We benchmark various mRAG methods with leading MLLMs on Dyn-VQA, demonstrating their flaw in providing sufficient and relevant knowledge for dynamic questions.
            </p>
            <p>
              🤖 We propose <b style="color:#615ced;">OmniSearch</b>, a self-adaptive retrieval agent that plans each retrieval action in realtime according to question solution stage and current retrieval content.
            </p>
            <p>
              📊 Extensive experiments prove the effectiveness of our OmniSearch. Detailed analyses are conducted to provide direction for advancing mRAG.
            </p>
          </div>
        </div>
      </div>
  </section>

<div class="content has-text-justified">
  <!-- Framework -->
  <section class="section" id="Framework">
    <div class="container is-max-desktop content">
      <div class="columns is-centered has-text-centered">
        <div class="column is-five-fifths">
          <h2 class="title is-3">📚Dyn-VQA</h2>
          <img src="static/images/example.png" width="80%">
          <div class="content has-text-justified">
            <p>
              <b style="color:#615ced;">Construction of Dyn-VQA Dataset.</b> The Dyn-VQA dataset is constructed to evaluate mRAG systems with dynamic questions requiring complex retrieval strategies. It consists of 1,452 questions built through three steps:
            </p>
            <ul>
              <li><b style="color:#ab42ab;">Step 1: Textual Question Writing:</b> Annotators create questions categorized by answer update frequency, need for external visual knowledge, and reasoning steps.</li>
              <li><b style="color:#ab42ab;">Step 2: Multimodal Rewriting:</b> Questions are converted to multimodal format by replacing visual references and pairing with relevant images.</li>
              <li><b style="color:#ab42ab;">Step 3: Translation:</b> Chinese and English versions are translated and verified for accuracy.</li>
            </ul>
          </div>
          <img src="static/images/dataset.png" width="100%">
          <div class="content has-text-justified">
            <p>
              <b style="color:#615ced;">Data Statistics.</b> The Dyn-VQA dataset comprises 1,452 questions across 9 domains, with an equal distribution between Chinese (50.8%) and English (49.2%). Questions are categorized by their answer update frequency: 26.5% have rapidly changing answers, 34.0% have slowly changing answers, and 39.5% have static answers. In terms of reasoning complexity, 73.3% of the questions require ≤ 2 reasoning steps, while 26.7% need more than 2 steps. Additionally, 59.6% of the questions require external visual knowledge, highlighting the multimodal nature of the dataset. The average question length is 12.5 tokens, and the average answer length is 4.3 tokens. Dyn-VQA is designed to be highly challenging, emphasizing dynamic and complex retrieval needs.
              <p style="text-align: center;">
                <img src="static/images/dataset2.png" width="50%">
              </p>
              <b style="color:#615ced;">Data Domain.</b> The Dyn-VQA dataset spans 9 domains. Sports and Recreation, along with Companies and Products, constitute approximately 50% of the data. The distribution of questions with fast, slow, and never-changing answers is relatively balanced among the categories and does not exhibit a long tail, reflecting a distribution that closely aligns with real-world scenarios.

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Overview -->
  <!-- Framework -->
  <section class="section" id="Framework">
    <div class="container is-max-desktop content">
      <div class="columns is-centered has-text-centered">
        <div class="column is-five-fifths">
          <h2 class="title is-3">🤖OmniSearch</h2>
          <img src="static/images/method.png" width="90%">
          <div class="content has-text-justified">
          <p>
            The OmniSearch framework operates through the collaboration of three key modules to effectively tackle complex multimodal questions:
          </p>

            <p>
              <b style="color:#615ced;">Planning Agent.</b> This module understands the input question and real-world feedback, formulates sub-questions, and plans the next retrieval action by selecting the appropriate API and query based on the required knowledge type. It generates various potential actions, such as clarifying ambiguities, refining queries, and proposing next steps.
            </p>
            <p>
              <b style="color:#615ced;">Retriever.</b> This module executes the actual retrieval operations using the chosen API and query. It fetches relevant content from external sources, including web searches and image searches, providing the necessary information to address the sub-questions.
            </p>
            <p>
              <b style="color:#615ced;">Sub-question Solver.</b>  This module summarizes the retrieved content and attempts to answer the sub-question. It processes the information from the retriever and generates feedback for the planning agent, helping it assess the adequacy of the retrieved content and decide on the next steps.
            </p>
            <p>
              OmniSearch iterates through these steps, dynamically adjusting its retrieval strategy based on feedback until it gathers sufficient knowledge to provide a comprehensive final answer to the original question.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Framework -->

<!-- Experiments -->
<section class="section" id="Experiments">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">📊Experiments</h2>
        <div class="content has-text-justified">
        </div>
        <img src="static/images/agent_result.png" width="80%">
        
        <div class="content has-text-justified">
          <p>
            <b style="color:#615ced;">Main Results.</b> 
            OmniSearch (GPT-4V) outperforms other models by breaking down complex questions into sub-questions and reassessing content to ensure accuracy, reducing error propagation. While it matches human-level performance in some aspects, it still struggles with challenging questions that require fast-changing knowledge, multiple retrieval steps, or external visual knowledge. Two-step heuristic mRAG aids in detailed image descriptions, although its advantage is limited to questions needing visual input. Commercial search engines like Gemini lack grounding capabilities essential for effective multimodal integration. Finally, mRAG helps bridge the performance gap between text and multimodal models.
          </p>
          <p>
            <b style="color:#615ced;">How different models as sub-question solvers affect token and expenses?</b> 
            We analyze how different sub-question solvers impact token costs and expenses. Despite higher costs, OmniSearch significantly outperforms heuristic mRAG, with a proportional but non-linear relationship between OmniSearch's performance and its costs. Replacing GPT-4V with Qwen-VL-Chat reduces performance by less than 4 points (around 7.9%) but nearly halves expenses, demonstrating OmniSearch's scalability. Sub-question reasoning isn't the main bottleneck; rather, improving retrieval strategies for complex questions is more critical. This is evident from the significant benefits of changing the planning model compared to altering the sub-question solver in OmniSearch (Q) with GPT-4V. Thus, with limited computational resources, prioritizing a larger model for retrieval planning is advisable.
          </p>
        </div>
      </div>
    </div>
    <img src="static/images/cost.png" width="80%">
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
              target="_blank">Academic Project Page Template</a> which was adopted from the <a
              href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer.
            <br> This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>

        <!-- 添加一个居中图片的div -->
        <div style="text-align: center;">
          <img src="your-image-url.jpg" alt="Description of image" style="max-width: 100%; height: auto;">
        </div>
        
      </div>
    </div>
  </div>
</footer>


  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
